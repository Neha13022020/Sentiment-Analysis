{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_preprocessing_and_modeling.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "KpjYF8S1UU0q"
      ],
      "authorship_tag": "ABX9TyNC0A0LKpSWMvryWnY/toFx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Neha13022020/Sentiment-Analysis/blob/main/text_preprocessing_and_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMSwDtJ1MRla",
        "outputId": "89354c87-defd-429f-a454-fe87dede8f13"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('gdrive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOvIme-WMTRr"
      },
      "source": [
        "# Importing libraries\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import re\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "from tqdm import tqdm\r\n",
        "\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "\r\n",
        "from gensim.models import Word2Vec\r\n",
        "from gensim.models import KeyedVectors\r\n",
        "import pickle\r\n",
        "\r\n",
        "import os"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpjYF8S1UU0q"
      },
      "source": [
        "## Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VFF1RoTMUcn",
        "outputId": "e5ab7eca-3660-4864-ed8f-11448486069d"
      },
      "source": [
        "df = pd.read_csv('/content/gdrive/MyDrive/Forsk_assignment/balanced_review.csv')\r\n",
        "print(df.shape)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1188000, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2M-cmw_VMUZQ",
        "outputId": "df97ca3e-cb37-41c8-be0a-df9da473f64a"
      },
      "source": [
        "print(df.isna().sum())"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "overall         0\n",
            "reviewText    939\n",
            "summary       342\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ly6lsgmvMUWs",
        "outputId": "896ef30a-b14c-4a3d-e9fb-102da4b279ab"
      },
      "source": [
        "# dropping null values\r\n",
        "\r\n",
        "df = df.dropna()\r\n",
        "print(df.shape)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1186734, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1MQsywsMUUL",
        "outputId": "ff8a7ee1-de2e-4d6e-f3ce-ee6092a05c74"
      },
      "source": [
        "# converting reviews into 2 classes : 0 ---> Bad, 1 ---> Good \r\n",
        "\r\n",
        "df = df[df['overall'] != 3]\r\n",
        "df['Positivity'] = np.where(df['overall'] > 3, 1, 0 )\r\n",
        "\r\n",
        "print(df['Positivity'].value_counts())\r\n",
        "df.shape"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    395683\n",
            "1    395305\n",
            "Name: Positivity, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(790988, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IonITlmO-Xn"
      },
      "source": [
        "# https://gist.github.com/sebleier/554280\r\n",
        "# we are removing the words from the stop words list: 'no', 'nor', 'not'\r\n",
        "# <br /><br /> ==> after the above steps, we are getting \"br br\"\r\n",
        "# we are including them into stop words list\r\n",
        "# instead of <br /> if we have <br/> these tags would have revmoved in the 1st step\r\n",
        "\r\n",
        "stopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\r\n",
        "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\r\n",
        "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\r\n",
        "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\r\n",
        "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\r\n",
        "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\r\n",
        "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\r\n",
        "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\r\n",
        "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\r\n",
        "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\r\n",
        "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\r\n",
        "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\r\n",
        "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\r\n",
        "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\r\n",
        "            'won', \"won't\", 'wouldn', \"wouldn't\"])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylgrpjegMUO3"
      },
      "source": [
        "# https://stackoverflow.com/a/47091490/4084039\r\n",
        "\r\n",
        "\r\n",
        "def decontracted(phrase):\r\n",
        "    # specific\r\n",
        "    phrase = re.sub(r\"won't\", \"will not\", phrase)\r\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\r\n",
        "\r\n",
        "    # general\r\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\r\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\r\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\r\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\r\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\r\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\r\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\r\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\r\n",
        "    return phrase"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggLQZnf6Nu1R",
        "outputId": "91b7769c-e6aa-46cf-8e0d-84ff2060f73b"
      },
      "source": [
        "preprocessed_reviewText = []\r\n",
        "preprocessed_summary = []\r\n",
        "\r\n",
        "for sentance in tqdm(df['reviewText'].values):\r\n",
        "    sentance = re.sub(r\"http\\S+\", \"\", sentance)\r\n",
        "    sentance = BeautifulSoup(sentance, 'lxml').get_text()\r\n",
        "    sentance = decontracted(sentance)\r\n",
        "    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\r\n",
        "    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\r\n",
        "    # https://gist.github.com/sebleier/554280\r\n",
        "    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stopwords)\r\n",
        "    preprocessed_reviewText.append(sentance.strip())\r\n",
        "\r\n",
        "\r\n",
        "for sentance in tqdm(df['summary'].values):\r\n",
        "    sentance = re.sub(r\"http\\S+\", \"\", sentance)\r\n",
        "    sentance = BeautifulSoup(sentance, 'lxml').get_text()\r\n",
        "    sentance = decontracted(sentance)\r\n",
        "    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\r\n",
        "    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\r\n",
        "    # https://gist.github.com/sebleier/554280\r\n",
        "    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stopwords)\r\n",
        "    preprocessed_summary.append(sentance.strip())\r\n",
        "\r\n",
        "len(preprocessed_summary),len(preprocessed_reviewText)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 20%|█▉        | 154668/790988 [00:40<02:48, 3775.84it/s]/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:273: UserWarning: \"b'/'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
            "  ' Beautiful Soup.' % markup)\n",
            " 77%|███████▋  | 605140/790988 [02:42<00:54, 3408.04it/s]/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:273: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
            "  ' Beautiful Soup.' % markup)\n",
            "100%|██████████| 790988/790988 [03:35<00:00, 3675.45it/s]\n",
            "  6%|▌         | 45474/790988 [00:10<02:50, 4368.34it/s]/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:273: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
            "  ' Beautiful Soup.' % markup)\n",
            " 22%|██▏       | 176764/790988 [00:46<02:20, 4361.94it/s]/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:273: UserWarning: \"b'//'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
            "  ' Beautiful Soup.' % markup)\n",
            "100%|██████████| 790988/790988 [03:03<00:00, 4299.47it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(790988, 790988)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb-1kKpYNux9",
        "outputId": "92a41829-08a7-4f81-dc36-acedecbb7182"
      },
      "source": [
        "df['preprocessed_summary']=preprocessed_summary\r\n",
        "\r\n",
        "df['preprocessed_reviewText']=preprocessed_reviewText\r\n",
        "\r\n",
        "df = df.drop(['reviewText','summary'],axis=1)\r\n",
        "df.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(790988, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwgJ3-E3T7Ti"
      },
      "source": [
        "df.to_csv('/content/gdrive/MyDrive/Forsk_assignment/preprocessed_balanced_reviews.csv', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYtsQGncUzPv"
      },
      "source": [
        "## Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kw7QNKf_WVtk"
      },
      "source": [
        "df = pd.read_csv('/content/gdrive/MyDrive/Forsk_assignment/preprocessed_balanced_reviews.csv')"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNFxqDmdNuqp",
        "outputId": "4d8050ae-87df-4084-f805-14a49652c1c7"
      },
      "source": [
        "df = df.dropna()\r\n",
        "print(df.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(790988, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQFRmD8Z1jwa",
        "outputId": "5315ad2b-1af7-45c0-bf02-d1c93ba73475"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['overall', 'Positivity', 'preprocessed_summary',\n",
              "       'preprocessed_reviewText'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8p-GYjyS18g",
        "outputId": "c778512a-775e-42cd-a0f0-e6294d660f14"
      },
      "source": [
        "# avg-w2v for reviews\r\n",
        "\r\n",
        "sentance_review =[]\r\n",
        "for sentance in df['preprocessed_reviewText']:\r\n",
        "    sentance_review.append(sentance.split())\r\n",
        "\r\n",
        "w2v_model = Word2Vec(sentance_review,min_count=5,size=50, workers=4)\r\n",
        "w2v_words = list(w2v_model.wv.vocab)\r\n",
        "\r\n",
        "sent_vectors_r = [] # the avg-w2v for each sentence/review is stored in this list\r\n",
        "for sent in tqdm(sentance_review): # for each review/sentence\r\n",
        "    sent_vec = np.zeros(50) # as word vectors are of zero length 50, you might need to change this to 300 if you use google's w2v\r\n",
        "    cnt_words =0 # num of words with a valid vector in the sentence/review\r\n",
        "    for word in sent: # for each word in a review/sentence\r\n",
        "        if word in w2v_words:\r\n",
        "            vec = w2v_model.wv[word]\r\n",
        "            sent_vec += vec\r\n",
        "            cnt_words += 1\r\n",
        "    if cnt_words != 0:\r\n",
        "        sent_vec /= cnt_words\r\n",
        "    sent_vectors_r.append(sent_vec)\r\n",
        "\r\n",
        "print(len(sent_vectors_r))\r\n",
        "print(len(sent_vectors_r[0]))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 790988/790988 [15:35<00:00, 845.78it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "790988\n",
            "50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVaIX_CW09N-"
      },
      "source": [
        "w2v_model.save(\"/content/gdrive/MyDrive/Forsk_assignment/word2vec.model\")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gCYCLMtXMPE",
        "outputId": "2c7c926e-59a9-42bf-8639-ea5cf8aed4cf"
      },
      "source": [
        "# avg-w2v for summary\r\n",
        "\r\n",
        "sentance_summary =[]\r\n",
        "for sentance in df['preprocessed_summary']:\r\n",
        "    sentance_summary.append(sentance.split())\r\n",
        "\r\n",
        "w2v_model_2 = Word2Vec(sentance_summary,min_count=5,size=50, workers=4)\r\n",
        "w2v_words = list(w2v_model_2.wv.vocab)\r\n",
        "\r\n",
        "sent_vectors_s = [] # the avg-w2v for each sentence/review is stored in this list\r\n",
        "for sent in tqdm(sentance_summary): # for each review/sentence\r\n",
        "    sent_vec = np.zeros(50) # as word vectors are of zero length 50, you might need to change this to 300 if you use google's w2v\r\n",
        "    cnt_words =0 # num of words with a valid vector in the sentence/review\r\n",
        "    for word in sent: # for each word in a review/sentence\r\n",
        "        if word in w2v_words:\r\n",
        "            vec = w2v_model_2.wv[word]\r\n",
        "            sent_vec += vec\r\n",
        "            cnt_words += 1\r\n",
        "    if cnt_words != 0:\r\n",
        "        sent_vec /= cnt_words\r\n",
        "    sent_vectors_s.append(sent_vec)\r\n",
        "\r\n",
        "print(len(sent_vectors_s))\r\n",
        "print(len(sent_vectors_s[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 789108/789108 [01:13<00:00, 10723.73it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "789108\n",
            "50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rGVRAWgXMLp",
        "outputId": "50e082b7-5006-4377-8766-143625172daa"
      },
      "source": [
        "sent_vectors_r = pd.DataFrame(sent_vectors_r)\r\n",
        "sent_vectors_s = pd.DataFrame(sent_vectors_s)\r\n",
        "print(sent_vectors_r.shape,sent_vectors_s.shape)\r\n",
        "\r\n",
        "new_df = pd.concat([sent_vectors_r,sent_vectors_s], axis=1, ignore_index = True)\r\n",
        "new_df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(789108, 50) (789108, 50)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(789108, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R44k7hBtXMJH",
        "outputId": "9146461f-f7f9-4e62-e3fd-6a46958694ba"
      },
      "source": [
        "t = []\r\n",
        "for i in df['Positivity']:\r\n",
        "  t.append(i)\r\n",
        "new_df['Target'] = t\r\n",
        "new_df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(789108, 101)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlGGWA4KXMGc"
      },
      "source": [
        "new_df.to_csv('/content/gdrive/MyDrive/Forsk_assignment/vectorized_balanced_reviews.csv', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Yos179gAkHN"
      },
      "source": [
        "## Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOcoQ4mdDxr4"
      },
      "source": [
        "new_df = pd.read_csv('/content/gdrive/MyDrive/Forsk_assignment/vectorized_balanced_reviews.csv')"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5wgHlciXMD3",
        "outputId": "08ef12d7-f912-49dc-8802-656bb23c51cc"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "Target = new_df['Target']\r\n",
        "new_df = new_df.drop(['Target'],axis=1)\r\n",
        "\r\n",
        "features_train, features_test, labels_train, labels_test = train_test_split(new_df, Target, random_state = 42) \r\n",
        "features_train.shape, features_test.shape, labels_train.shape, labels_test.shape"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((591831, 100), (197277, 100), (591831,), (197277,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkcOLk6hXL70",
        "outputId": "c7b5ee96-43ad-4a24-deed-acd3d644f127"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\r\n",
        "\r\n",
        "model = LogisticRegression()\r\n",
        "model.fit(features_train, labels_train)\r\n",
        "\r\n",
        "\r\n",
        "perdictions = model.predict(features_test)\r\n",
        "\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "\r\n",
        "print(confusion_matrix(labels_test, perdictions))\r\n",
        "\r\n",
        "from sklearn.metrics import roc_auc_score\r\n",
        "roc_auc_score(labels_test, perdictions)\r\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[90135  8346]\n",
            " [ 9766 89030]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9082012662295326"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7ACz0suD3NQ"
      },
      "source": [
        "# saving the model\r\n",
        "\r\n",
        "import pickle\r\n",
        "\r\n",
        "pkl_filename = \"/content/gdrive/MyDrive/Forsk_assignment/pickle_model.pkl\"\r\n",
        "\r\n",
        "with open(pkl_filename, 'wb') as file:\r\n",
        "    pickle.dump(model, file)\r\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJfe8XWwD24Q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}